ROOFLYTICS – TRAINING THEORY NOTES
(DEEP LEARNING FOUNDATIONS & MATHEMATICAL BASIS)
================================================

This document presents the mathematical and theoretical foundations
underlying the Rooflytics training and inference pipeline.

It focuses on:
• Semantic segmentation theory
• Loss functions and optimization
• CNN and encoder–decoder architectures
• Transfer learning
• Numerical stability
• Clustering mathematics
• Energy model dimensional correctness

================================================
1. BINARY SEGMENTATION FORMULATION
================================================

Let the input RGB image be:
X ∈ ℝ^(H × W × 3)

Ground truth roof mask:
Y ∈ {0,1}^(H × W)

The segmentation model f_θ maps:
f_θ(X) → Ŷ ∈ [0,1]^(H × W)

Each pixel output ŷ_ij represents the probability that pixel (i,j)
belongs to the roof class.

This formulation treats segmentation as pixel-wise Bernoulli
classification.

================================================
2. BINARY CROSS-ENTROPY (BCE) LOSS
================================================

Binary Cross-Entropy loss is defined as:

L_BCE = - Σ [ y log(p) + (1 - y) log(1 - p) ]

where:
• y ∈ {0,1} is the ground truth label
• p ∈ (0,1) is the predicted probability

Gradient with respect to p:

∂L/∂p = (p - y) / [p(1 - p)]

BCE provides:
• probabilistic interpretation
• stable gradients for pixel-wise classification

However, BCE alone does not handle class imbalance well.

================================================
3. DICE COEFFICIENT
================================================

The Dice similarity coefficient measures overlap:

Dice = 2TP / (2TP + FP + FN)

For soft probabilistic outputs, the Soft Dice loss is used:

L_Dice = 1 - (2 Σ (p · y) + ε) / (Σ p + Σ y + ε)

where ε is a small constant for numerical stability.

Dice loss:
• directly optimizes overlap
• is robust to foreground–background imbalance
• complements BCE

================================================
4. COMBINED LOSS FUNCTION
================================================

Final loss used for training:

L_total = L_BCE + L_Dice

Rationale:
• BCE enforces pixel-wise accuracy
• Dice enforces region-level overlap

The combination stabilizes training and improves segmentation quality
on imbalanced roof datasets.

================================================
5. BACKPROPAGATION
================================================

Training minimizes the loss via gradient descent.

Using the chain rule:

∂L/∂θ = ∂L/∂ŷ · ∂ŷ/∂θ

Gradients are propagated layer-by-layer through:
• convolutional layers
• activation functions
• batch normalization
• skip connections

PyTorch automatically computes these gradients via autograd.

================================================
6. CONVOLUTION OPERATION
================================================

For a 2D convolution:

Output(i,j) = Σ_u Σ_v [ K(u,v) · X(i+u, j+v) ]

Where:
• K is the learnable kernel
• X is the input feature map

Convolutions enable:
• local receptive fields
• weight sharing
• translation invariance

These properties make CNNs suitable for spatial data such as imagery.

================================================
7. ENCODER–DECODER THEORY
================================================

Encoder:
• Gradually reduces spatial resolution
• Extracts high-level semantic features

Decoder:
• Upsamples feature maps
• Restores spatial resolution

Skip connections:

F_out = concat(F_encoder, F_decoder)

Benefits:
• Preserve fine spatial details
• Improve gradient flow
• Enable precise boundary localization

This structure is central to the U-Net architecture.

================================================
8. TRANSFER LEARNING
================================================

Model parameters are initialized as:

θ = θ_pretrained + Δθ

Where θ_pretrained comes from ImageNet-trained networks.

Benefits:
• Faster convergence
• Better generalization on small datasets
• Reduced risk of vanishing gradients

In Rooflytics:
• Scratch U-Net → correctness validation
• EfficientNet-backed U-Net → production inference

================================================
9. MIXED PRECISION TRAINING
================================================

Mixed precision uses float16 instead of float32 for selected operations.

Memory usage:
M_new ≈ M_old / 2

Advantages:
• Reduced VRAM usage
• Faster computation on modern GPUs
• Larger batch sizes possible

Implemented using:
torch.cuda.amp.autocast
torch.cuda.amp.GradScaler

================================================
10. EVALUATION METRICS
================================================

Intersection over Union (IoU):

IoU = |A ∩ B| / |A ∪ B|

Dice coefficient relation:

Dice = 2IoU / (1 + IoU)

Dice is preferred for segmentation evaluation due to its sensitivity
to overlap quality.

================================================
11. CLUSTERING MATHEMATICS (K-MEANS)
================================================

K-Means minimizes the objective function:

J = Σ_i ||x_i - μ_k||²

Where:
• x_i is a data point
• μ_k is the centroid of cluster k

Before clustering, features are standardized:

x' = (x - μ) / σ

Standardization is mandatory to:
• prevent scale dominance
• ensure fair clustering based on reflectance and area

================================================
12. ENERGY MODEL – DIMENSIONAL CONSISTENCY
================================================

Cooling-energy savings are estimated as:

E_saved = A × Δα × G × η × T × U

Where:
• A = roof area (m²)
• Δα = reflectance improvement (dimensionless)
• G = solar irradiance (kW/m²)
• η = cooling efficiency
• T = annual sunlight hours (hours)
• U = usage factor (dimensionless)

Unit check:

m² × kW/m² × hours = kWh

This confirms dimensional correctness of the energy model.

================================================
FINAL NOTE
================================================

These theoretical foundations ensure that:
• the ML model is mathematically sound
• optimization is stable
• clustering is meaningful
• energy estimates are physically consistent

Rooflytics integrates deep learning theory with
physics-aware modeling and practical engineering constraints.

------------------------------------------------
END OF DOCUMENT
------------------------------------------------
